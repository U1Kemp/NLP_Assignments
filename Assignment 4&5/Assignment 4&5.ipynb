{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "7jMT5SBNwubn"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Utpalraj Kemprai"
      ],
      "metadata": {
        "id": "5F_fqnd1wpg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing necessary libraries"
      ],
      "metadata": {
        "id": "7jMT5SBNwubn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0W_bjwy8T1s"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import string\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pickle\n",
        "import gdown\n",
        "import zipfile\n",
        "import multiprocessing\n",
        "from concurrent.futures import ProcessPoolExecutor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 4"
      ],
      "metadata": {
        "id": "pRuyFX7k2dq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_id = '1PVA5IR4bAn7RulAeuSgxRBNMtmRGXaUm'"
      ],
      "metadata": {
        "id": "HS451Wcz-gDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading the corpus"
      ],
      "metadata": {
        "id": "SARIs4sUw7Et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', 'partial_dataset.zip', quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "A_zX_7M--4sh",
        "outputId": "d2389795-f84c-4a78-ef16-a3d4f376b8d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1PVA5IR4bAn7RulAeuSgxRBNMtmRGXaUm\n",
            "From (redirected): https://drive.google.com/uc?id=1PVA5IR4bAn7RulAeuSgxRBNMtmRGXaUm&confirm=t&uuid=bb46892c-6871-4185-9a28-cf75531f9796\n",
            "To: /content/partial_dataset.zip\n",
            "100%|██████████| 1.00G/1.00G [00:16<00:00, 58.9MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'partial_dataset.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = 'partial_dataset.zip'\n",
        "extract_dir = 'JSON_files'"
      ],
      "metadata": {
        "id": "Bu1IBcGu-7xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting the corpus content from downloaded zip files"
      ],
      "metadata": {
        "id": "9urZdJpYxBhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_zip(zip_path, extract_dir):\n",
        "  '''\n",
        "  Extracts the zip file to the specified directory.\n",
        "\n",
        "  Args:\n",
        "    zip_path: The path to the zip file.\n",
        "    extract_dir: The directory to extract the zip file to.\n",
        "  '''\n",
        "  with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "num_cores = multiprocessing.cpu_count()\n",
        "\n",
        "with ProcessPoolExecutor(max_workers=num_cores) as executor:\n",
        "  executor.submit(extract_zip, zip_path, extract_dir)"
      ],
      "metadata": {
        "id": "YvyXFBzl_AhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(filename):\n",
        "    '''\n",
        "    Reads a JSON file and returns its content as a string.\n",
        "\n",
        "    Args:\n",
        "      filename: The path to the JSON file.\n",
        "\n",
        "    Returns:\n",
        "      The content of the JSON file as a string.\n",
        "    '''\n",
        "    with open(filename, 'r') as f:\n",
        "        paper_content = json.load(f)\n",
        "    abstract = ''\n",
        "    if 'abstract' in paper_content:\n",
        "      for content in paper_content['abstract']:\n",
        "        if 'text' in content:\n",
        "          abstract += content['text']\n",
        "\n",
        "    return abstract\n",
        "\n",
        "def read_files(directory):\n",
        "    '''\n",
        "    Reads all JSON files in a directory and returns their contents as a list of strings.\n",
        "\n",
        "    Args:\n",
        "      directory: The path to the directory containing the JSON files.\n",
        "\n",
        "    Yields:\n",
        "      The content of each JSON file as a string.\n",
        "    '''\n",
        "    filenames = (os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.json'))\n",
        "    with ProcessPoolExecutor() as executor:\n",
        "        # Using map to process files in parallel\n",
        "        for result in tqdm(executor.map(read_file, filenames)):\n",
        "            yield result"
      ],
      "metadata": {
        "id": "cFlHTWhQ_EKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the corpus"
      ],
      "metadata": {
        "id": "5eJPhNp5xPHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_corpus = list(read_files('JSON_files/pdf_json/'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyHZADom_aqK",
        "outputId": "5d0ac72d-6f16-4a71-836c-0234864d38bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "56528it [01:02, 899.76it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_size = len(text_corpus)\n",
        "print(f'Corpus size: {corpus_size}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4h888GT_pxA",
        "outputId": "8f133470-2a8b-4deb-b042-ccc51d6de7fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus size: 56528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing the corpus"
      ],
      "metadata": {
        "id": "KSZWvO8A2rAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RRkALsh_iPP",
        "outputId": "66b40d89-aa74-4579-96f7-4dc2ecad5529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def casefolding(text):\n",
        "  '''\n",
        "  Converts the given text to lowercase.\n",
        "\n",
        "  Args:\n",
        "    text: The input text.\n",
        "\n",
        "  Returns:\n",
        "    The text in lowercase.\n",
        "  '''\n",
        "  return text.lower()"
      ],
      "metadata": {
        "id": "Ma2d9k07_7sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(text):\n",
        "  '''\n",
        "  Tokenizes the given text.\n",
        "\n",
        "  Args:\n",
        "    text: The input text.\n",
        "\n",
        "  Returns:\n",
        "    A list of tokens.\n",
        "  '''\n",
        "  return word_tokenize(text)"
      ],
      "metadata": {
        "id": "frAmKDSI_-ZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_special_chars_and_numbers_from_tokenized_text(tokens, min_count = 2):\n",
        "  \"\"\"\n",
        "  Removes numbers and special characters from a list of tokens. And then removes tokens that appear less than min_count times.\n",
        "\n",
        "  Args:\n",
        "    tokens: A list of tokens.\n",
        "\n",
        "  Returns:\n",
        "    A new list of tokens with numbers and special characters removed.\n",
        "  \"\"\"\n",
        "  filtered_tokens = [token for token in tokens if (not re.findall(\"[^\\u0000-\\u05C0\\u2100-\\u214F]+\",token)) and token.isalpha() and tokens.count(token) >= min_count and len(token) > 2]\n",
        "  return filtered_tokens"
      ],
      "metadata": {
        "id": "tmpJpiGiAB2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "JYvnn7SLAE74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "  '''\n",
        "  Preprocesses the text by removing special characters and numbers, converting to lowercase,\n",
        "  and tokenizing the text.\n",
        "\n",
        "  Args:\n",
        "    text: The text to be preprocessed.\n",
        "\n",
        "  Returns:\n",
        "    A list of preprocessed tokens.\n",
        "  '''\n",
        "  text = text.lower()\n",
        "  tokens = tokenize_text(text)\n",
        "  tokens = remove_special_chars_and_numbers_from_tokenized_text(tokens)\n",
        "  tokens = [token for token in tokens if token not in stop_words]\n",
        "  return tokens\n",
        "\n",
        "def preprocess_corpus(corpus):\n",
        "  '''\n",
        "  Preprocesses the corpus by applying the preprocess_text function to each document in parallel.\n",
        "\n",
        "  Args:\n",
        "    corpus: A list of documents.\n",
        "\n",
        "  Returns:\n",
        "    A list of preprocessed documents.\n",
        "  '''\n",
        "  with ProcessPoolExecutor() as executor:\n",
        "    return list(tqdm(executor.map(preprocess_text, corpus), total=corpus_size))"
      ],
      "metadata": {
        "id": "RD5G8UNlAL7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing the corpus"
      ],
      "metadata": {
        "id": "PdGtShCx5Kqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_corpus = preprocess_corpus(text_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFjfIWPrAahO",
        "outputId": "18de51ed-5319-4219-a1dc-aff2cb55be56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 56528/56528 [02:29<00:00, 379.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only considering documents with more than 300 words"
      ],
      "metadata": {
        "id": "MsuBx5h_YPbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_corpus_filtered = [doc for doc in tokenized_corpus if len(doc) > 300]"
      ],
      "metadata": {
        "id": "3smczyCwLzkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocabulary(tokenized_corpus):\n",
        "  '''\n",
        "  Gets the vocabulary of the given tokenized corpus.\n",
        "\n",
        "  Args:\n",
        "    tokenized_corpus: A list of tokenized texts.\n",
        "\n",
        "  Returns:\n",
        "    A set containing the unique words in the corpus.\n",
        "  '''\n",
        "  vocabulary = set()\n",
        "  for document in tqdm(tokenized_corpus):\n",
        "    for token in document:\n",
        "      vocabulary.add(token)\n",
        "  return vocabulary"
      ],
      "metadata": {
        "id": "tghmxA9uAgb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the vocabulary of the data"
      ],
      "metadata": {
        "id": "O_wVzYeg5RtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = get_vocabulary(tokenized_corpus_filtered)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wwmpkWxBAON",
        "outputId": "d0043f36-7d59-48e1-dd0b-79478385bb16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 506/506 [00:00<00:00, 15622.74it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size = len(vocabulary)\n",
        "print(f'Vocabulary size: {vocabulary_size}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csNmk3otBMKr",
        "outputId": "923696a8-651c-4daa-9a69-c1eda2f1d3c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 14171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = tuple(vocabulary)"
      ],
      "metadata": {
        "id": "8HGtST0FBQHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index = {word: i for i, word in enumerate(vocabulary)}"
      ],
      "metadata": {
        "id": "htnrY-zpBY7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "q5O8LvRQBvGZ",
        "outputId": "070a3e16-03f9-4faf-cb0e-ceb36b22dc75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('analytical',\n",
              " 'stirred',\n",
              " 'goes',\n",
              " 'operative',\n",
              " 'lod',\n",
              " 'snapshots',\n",
              " 'correlation',\n",
              " 'brefeldin',\n",
              " 'released',\n",
              " 'polymorphismus',\n",
              " 'schlafen',\n",
              " 'assumed',\n",
              " 'incubated',\n",
              " 'deletion',\n",
              " 'einem',\n",
              " 'zonder',\n",
              " 'evolved',\n",
              " 'arc',\n",
              " 'miss',\n",
              " 'behaviors',\n",
              " 'exhibited',\n",
              " 'tween',\n",
              " 'substitution',\n",
              " 'subject',\n",
              " 'leaders',\n",
              " 'picco',\n",
              " 'malrotation',\n",
              " 'toronto',\n",
              " 'frameshifting',\n",
              " 'macrodomain',\n",
              " 'épuration',\n",
              " 'bds',\n",
              " 'robotics',\n",
              " 'photochemical',\n",
              " 'cumulative',\n",
              " 'untersuchung',\n",
              " 'participatory',\n",
              " 'ozharvest',\n",
              " 'pcd',\n",
              " 'appearance',\n",
              " 'fuses',\n",
              " 'participation',\n",
              " 'catalysis',\n",
              " 'weighting',\n",
              " 'modulus',\n",
              " 'discriminating',\n",
              " 'rabbit',\n",
              " 'attenuate',\n",
              " 'consequences',\n",
              " 'changepoints',\n",
              " 'facilitate',\n",
              " 'dock',\n",
              " 'supervisor',\n",
              " 'verfü',\n",
              " 'darstellt',\n",
              " 'benefiting',\n",
              " 'abrasive',\n",
              " 'cardiovasculaire',\n",
              " 'monoinfection',\n",
              " 'flavonoids',\n",
              " 'advocates',\n",
              " 'fälle',\n",
              " 'drugbank',\n",
              " 'itaas',\n",
              " 'vast',\n",
              " 'verwendet',\n",
              " 'gluten',\n",
              " 'responsive',\n",
              " 'concentric',\n",
              " 'autodock',\n",
              " 'echocardiographic',\n",
              " 'kortexschichten',\n",
              " 'prodrug',\n",
              " 'fuliginosus',\n",
              " 'osteoporosis',\n",
              " 'heme',\n",
              " 'roughly',\n",
              " 'karaming',\n",
              " 'safa',\n",
              " 'lis',\n",
              " 'daytime',\n",
              " 'eingeschlossen',\n",
              " 'video',\n",
              " 'good',\n",
              " 'charakterisierung',\n",
              " 'nothing',\n",
              " 'bes',\n",
              " 'determining',\n",
              " 'spheroid',\n",
              " 'arch',\n",
              " 'nonapeptide',\n",
              " 'isoniazid',\n",
              " 'atmosphere',\n",
              " 'styrene',\n",
              " 'detectr',\n",
              " 'dilution',\n",
              " 'fibers',\n",
              " 'amplified',\n",
              " 'governments',\n",
              " 'owned',\n",
              " 'graph',\n",
              " 'solubility',\n",
              " 'pic',\n",
              " 'east',\n",
              " 'insecurity',\n",
              " 'humidified',\n",
              " 'differs',\n",
              " 'wicht',\n",
              " 'included',\n",
              " 'weisen',\n",
              " 'retamal',\n",
              " 'körpergewicht',\n",
              " 'optimising',\n",
              " 'svr',\n",
              " 'running',\n",
              " 'vabret',\n",
              " 'pneumoperitoneum',\n",
              " 'failures',\n",
              " 'nlr',\n",
              " 'fna',\n",
              " 'tooling',\n",
              " 'bereits',\n",
              " 'societal',\n",
              " 'lankan',\n",
              " 'olfactory',\n",
              " 'lymphknotengruppen',\n",
              " 'aetiology',\n",
              " 'unterschiedlichen',\n",
              " 'review',\n",
              " 'release',\n",
              " 'ageing',\n",
              " 'organelle',\n",
              " 'synapse',\n",
              " 'rules',\n",
              " 'cantor',\n",
              " 'mother',\n",
              " 'mejias',\n",
              " 'unexposed',\n",
              " 'engineers',\n",
              " 'thyroid',\n",
              " 'mst',\n",
              " 'studenten',\n",
              " 'organspende',\n",
              " 'hypovolemia',\n",
              " 'strengthen',\n",
              " 'junge',\n",
              " 'violet',\n",
              " 'nitrate',\n",
              " 'eqn',\n",
              " 'figure',\n",
              " 'auffällig',\n",
              " 'percepción',\n",
              " 'mind',\n",
              " 'cutaneous',\n",
              " 'needle',\n",
              " 'nomo',\n",
              " 'tasks',\n",
              " 'late',\n",
              " 'iugr',\n",
              " 'atu',\n",
              " 'deutschen',\n",
              " 'phase',\n",
              " 'vaccines',\n",
              " 'melting',\n",
              " 'property',\n",
              " 'aspartic',\n",
              " 'bar',\n",
              " 'banks',\n",
              " 'morgan',\n",
              " 'hypothetical',\n",
              " 'cardiomyocyte',\n",
              " 'prednisolone',\n",
              " 'rrt',\n",
              " 'constant',\n",
              " 'mounted',\n",
              " 'daten',\n",
              " 'turn',\n",
              " 'uorfs',\n",
              " 'outlets',\n",
              " 'dislocation',\n",
              " 'consumers',\n",
              " 'organelles',\n",
              " 'demonstrating',\n",
              " 'contribute',\n",
              " 'nachgewiesen',\n",
              " 'immature',\n",
              " 'rbd',\n",
              " 'anguilla',\n",
              " 'hemmung',\n",
              " 'methacrylate',\n",
              " 'null',\n",
              " 'srcr',\n",
              " 'outflow',\n",
              " 'transporters',\n",
              " 'robustness',\n",
              " 'master',\n",
              " 'aen',\n",
              " 'erj',\n",
              " 'zeigten',\n",
              " 'cbmo',\n",
              " 'tailed',\n",
              " 'underserved',\n",
              " 'synergy',\n",
              " 'disposable',\n",
              " 'least',\n",
              " 'dashed',\n",
              " 'pneumovax',\n",
              " 'svps',\n",
              " 'begomoviral',\n",
              " 'spricht',\n",
              " 'braken',\n",
              " 'ear',\n",
              " 'emd',\n",
              " 'nmr',\n",
              " 'amplitude',\n",
              " 'wichtige',\n",
              " 'lego',\n",
              " 'nakatira',\n",
              " 'yamanashi',\n",
              " 'cerebral',\n",
              " 'gat',\n",
              " 'ästen',\n",
              " 'soluble',\n",
              " 'paid',\n",
              " 'behave',\n",
              " 'venturi',\n",
              " 'sizes',\n",
              " 'wlth',\n",
              " 'quantification',\n",
              " 'spenderorgan',\n",
              " 'tnf',\n",
              " 'lifescience',\n",
              " 'rinse',\n",
              " 'common',\n",
              " 'free',\n",
              " 'mobility',\n",
              " 'cdna',\n",
              " 'spina',\n",
              " 'panama',\n",
              " 'throughout',\n",
              " 'retains',\n",
              " 'rain',\n",
              " 'optical',\n",
              " 'fetal',\n",
              " 'cortex',\n",
              " 'flotrac',\n",
              " 'conductors',\n",
              " 'reflect',\n",
              " 'tight',\n",
              " 'leicht',\n",
              " 'lifelong',\n",
              " 'tools',\n",
              " 'graves',\n",
              " 'saxs',\n",
              " 'classrooms',\n",
              " 'parenchymal',\n",
              " 'exhibits',\n",
              " 'determination',\n",
              " 'dieser',\n",
              " 'database',\n",
              " 'specialised',\n",
              " 'vrcs',\n",
              " 'showed',\n",
              " 'eigenen',\n",
              " 'noncommunicable',\n",
              " 'imf',\n",
              " 'mixture',\n",
              " 'tonsil',\n",
              " 'netzhaut',\n",
              " 'albopictus',\n",
              " 'seemingly',\n",
              " 'cellulitis',\n",
              " 'appearequipment',\n",
              " 'quarantined',\n",
              " 'eru',\n",
              " 'fas',\n",
              " 'konnte',\n",
              " 'contributions',\n",
              " 'hematopoiesis',\n",
              " 'fire',\n",
              " 'mus',\n",
              " 'anns',\n",
              " 'supernatants',\n",
              " 'methodology',\n",
              " 'recipient',\n",
              " 'adrenal',\n",
              " 'diagnostic',\n",
              " 'vieler',\n",
              " 'occupational',\n",
              " 'reflection',\n",
              " 'fao',\n",
              " 'petg',\n",
              " 'zeigen',\n",
              " 'palsy',\n",
              " 'technicians',\n",
              " 'salivaall',\n",
              " 'kliniken',\n",
              " 'lungs',\n",
              " 'alteration',\n",
              " 'racial',\n",
              " 'directions',\n",
              " 'timelines',\n",
              " 'sleep',\n",
              " 'optreden',\n",
              " 'friend',\n",
              " 'anim',\n",
              " 'neutrophilia',\n",
              " 'elderly',\n",
              " 'deshalb',\n",
              " 'sorted',\n",
              " 'almanach',\n",
              " 'configuration',\n",
              " 'fplc',\n",
              " 'converted',\n",
              " 'microstructure',\n",
              " 'laboratories',\n",
              " 'dovepress',\n",
              " 'leukozyten',\n",
              " 'sxb',\n",
              " 'kommt',\n",
              " 'neighbor',\n",
              " 'pufa',\n",
              " 'immunostaining',\n",
              " 'interleukin',\n",
              " 'sclerosing',\n",
              " 'clp',\n",
              " 'verschiedene',\n",
              " 'schneider',\n",
              " 'wong',\n",
              " 'cinahl',\n",
              " 'statistisch',\n",
              " 'ris',\n",
              " 'pascal',\n",
              " 'ksgn',\n",
              " 'physician',\n",
              " 'adulthood',\n",
              " 'kilifi',\n",
              " 'biogenesis',\n",
              " 'duodenal',\n",
              " 'detrimental',\n",
              " 'superalloy',\n",
              " 'bcg',\n",
              " 'inhibits',\n",
              " 'hairpin',\n",
              " 'ground',\n",
              " 'aust',\n",
              " 'treadmill',\n",
              " 'coding',\n",
              " 'proceed',\n",
              " 'involvement',\n",
              " 'worn',\n",
              " 'cytosol',\n",
              " 'stop',\n",
              " 'consist',\n",
              " 'erythematous',\n",
              " 'injected',\n",
              " 'natl',\n",
              " 'transmit',\n",
              " 'sir',\n",
              " 'micu',\n",
              " 'pcr',\n",
              " 'shortening',\n",
              " 'validate',\n",
              " 'vec',\n",
              " 'coat',\n",
              " 'jersey',\n",
              " 'tightly',\n",
              " 'organen',\n",
              " 'willebrand',\n",
              " 'majority',\n",
              " 'periventrikuläre',\n",
              " 'monate',\n",
              " 'water',\n",
              " 'greatest',\n",
              " 'terminal',\n",
              " 'immunize',\n",
              " 'goals',\n",
              " 'denoted',\n",
              " 'busco',\n",
              " 'rights',\n",
              " 'exemplified',\n",
              " 'probanden',\n",
              " 'rsm',\n",
              " 'dbd',\n",
              " 'morphologies',\n",
              " 'turned',\n",
              " 'calming',\n",
              " 'ranged',\n",
              " 'holzer',\n",
              " 'uas',\n",
              " 'bivalent',\n",
              " 'displays',\n",
              " 'strains',\n",
              " 'intravascular',\n",
              " 'submission',\n",
              " 'hematological',\n",
              " 'empirical',\n",
              " 'rlrs',\n",
              " 'subjected',\n",
              " 'inside',\n",
              " 'enveloped',\n",
              " 'interestingly',\n",
              " 'evidenced',\n",
              " 'kda',\n",
              " 'therapeutischen',\n",
              " 'month',\n",
              " 'frühe',\n",
              " 'hexnac',\n",
              " 'thousands',\n",
              " 'meron',\n",
              " 'dsd',\n",
              " 'weitere',\n",
              " 'remdesivir',\n",
              " 'actuators',\n",
              " 'chikv',\n",
              " 'len',\n",
              " 'neue',\n",
              " 'präkapillaren',\n",
              " 'mindestens',\n",
              " 'opacities',\n",
              " 'passengers',\n",
              " 'adapted',\n",
              " 'diffusion',\n",
              " 'perioperative',\n",
              " 'ionals',\n",
              " 'optimizer',\n",
              " 'fragmented',\n",
              " 'intrarenal',\n",
              " 'entweder',\n",
              " 'hrsn',\n",
              " 'recurrence',\n",
              " 'hyperactivated',\n",
              " 'zich',\n",
              " 'ihre',\n",
              " 'steroidogenesis',\n",
              " 'remodeling',\n",
              " 'identity',\n",
              " 'univariable',\n",
              " 'polyproteins',\n",
              " 'motivate',\n",
              " 'asawa',\n",
              " 'training',\n",
              " 'rndv',\n",
              " 'hard',\n",
              " 'probiotics',\n",
              " 'organoids',\n",
              " 'deviation',\n",
              " 'thrombogenic',\n",
              " 'cuatro',\n",
              " 'observed',\n",
              " 'hyperresponsiveness',\n",
              " 'workplace',\n",
              " 'evidencia',\n",
              " 'narrativa',\n",
              " 'inflammation',\n",
              " 'luxation',\n",
              " 'experiment',\n",
              " 'possono',\n",
              " 'taste',\n",
              " 'directed',\n",
              " 'cordis',\n",
              " 'invariably',\n",
              " 'king',\n",
              " 'scenarios',\n",
              " 'erreichen',\n",
              " 'absolute',\n",
              " 'uveitis',\n",
              " 'applying',\n",
              " 'finishing',\n",
              " 'prozesse',\n",
              " 'übertragungsweg',\n",
              " 'chga',\n",
              " 'dna',\n",
              " 'subgenotypes',\n",
              " 'convct',\n",
              " 'rapid',\n",
              " 'cytokines',\n",
              " 'erythrocyte',\n",
              " 'truncation',\n",
              " 'dedicated',\n",
              " 'concerning',\n",
              " 'per',\n",
              " 'savolainen',\n",
              " 'udaf',\n",
              " 'discrete',\n",
              " 'utilizes',\n",
              " 'ship',\n",
              " 'endocytic',\n",
              " 'behandelnde',\n",
              " 'tourists',\n",
              " 'turbulence',\n",
              " 'wheezing',\n",
              " 'ito',\n",
              " 'deconvolved',\n",
              " 'predicts',\n",
              " 'verneinung',\n",
              " 'epicardial',\n",
              " 'principal',\n",
              " 'moyen',\n",
              " 'described',\n",
              " 'chirurgischen',\n",
              " 'milling',\n",
              " 'bores',\n",
              " 'capacities',\n",
              " 'branch',\n",
              " 'processes',\n",
              " 'dysregulation',\n",
              " 'difference',\n",
              " 'pbmcs',\n",
              " 'manifested',\n",
              " 'aztreonam',\n",
              " 'hsaec',\n",
              " 'stretched',\n",
              " 'candidates',\n",
              " 'enclosed',\n",
              " 'breast',\n",
              " 'mem',\n",
              " 'drucksenkenden',\n",
              " 'fab',\n",
              " 'inefficiencies',\n",
              " 'intraosseous',\n",
              " 'sheltering',\n",
              " 'ibm',\n",
              " 'microbiomes',\n",
              " 'vein',\n",
              " 'planning',\n",
              " 'heartbeats',\n",
              " 'leisure',\n",
              " 'eqs',\n",
              " 'formulate',\n",
              " 'deaminases',\n",
              " 'angiogenesis',\n",
              " 'formal',\n",
              " 'update',\n",
              " 'vrnps',\n",
              " 'dispersed',\n",
              " 'medizinischen',\n",
              " 'balf',\n",
              " 'chen',\n",
              " 'iridium',\n",
              " 'pedagogy',\n",
              " 'diego',\n",
              " 'pdcs',\n",
              " 'geographic',\n",
              " 'kavaliers',\n",
              " 'teilnehmer',\n",
              " 'selected',\n",
              " 'body',\n",
              " 'fecha',\n",
              " 'sinha',\n",
              " 'pide',\n",
              " 'perimembranous',\n",
              " 'chicago',\n",
              " 'seinem',\n",
              " 'encountering',\n",
              " 'retinal',\n",
              " 'microvasculitis',\n",
              " 'stretching',\n",
              " 'cintrat',\n",
              " 'atrophieindex',\n",
              " 'acquisition',\n",
              " 'enzo',\n",
              " 'increased',\n",
              " 'transitions',\n",
              " 'resp',\n",
              " 'stewardship',\n",
              " 'subkutanen',\n",
              " 'finished',\n",
              " 'graphically',\n",
              " 'distant',\n",
              " 'shielding',\n",
              " 'infectivities',\n",
              " 'deficit',\n",
              " 'variety',\n",
              " 'niet',\n",
              " 'uuu',\n",
              " 'market',\n",
              " 'kills',\n",
              " 'physiology',\n",
              " 'integer',\n",
              " 'icu',\n",
              " 'hospitalizations',\n",
              " 'probability',\n",
              " 'room',\n",
              " 'cdns',\n",
              " 'supplemental',\n",
              " 'subgenus',\n",
              " 'biol',\n",
              " 'allowed',\n",
              " 'rfe',\n",
              " 'taguchi',\n",
              " 'río',\n",
              " 'panksepp',\n",
              " 'aes',\n",
              " 'fcrs',\n",
              " 'seven',\n",
              " 'pandemie',\n",
              " 'ccmv',\n",
              " 'diel',\n",
              " 'governing',\n",
              " 'respecto',\n",
              " 'αsα',\n",
              " 'trypanosomatidae',\n",
              " 'systematic',\n",
              " 'blot',\n",
              " 'communication',\n",
              " 'references',\n",
              " 'better',\n",
              " 'sitzung',\n",
              " 'mrna',\n",
              " 'nonexistent',\n",
              " 'several',\n",
              " 'smooth',\n",
              " 'regard',\n",
              " 'ursprünglich',\n",
              " 'rvl',\n",
              " 'care',\n",
              " 'autoantibodies',\n",
              " 'eitrigen',\n",
              " 'strain',\n",
              " 'convolutional',\n",
              " 'tek',\n",
              " 'entwickelten',\n",
              " 'cycloheptanone',\n",
              " 'program',\n",
              " 'various',\n",
              " 'commensal',\n",
              " 'meld',\n",
              " 'death',\n",
              " 'wei',\n",
              " 'mutually',\n",
              " 'continent',\n",
              " 'wala',\n",
              " 'bean',\n",
              " 'aufweisen',\n",
              " 'neurogenic',\n",
              " 'saudi',\n",
              " 'breeding',\n",
              " 'human',\n",
              " 'acidic',\n",
              " 'sortiert',\n",
              " 'steigende',\n",
              " 'nsr',\n",
              " 'insight',\n",
              " 'fluidic',\n",
              " 'immer',\n",
              " 'exerts',\n",
              " 'associative',\n",
              " 'ophthalmic',\n",
              " 'auf',\n",
              " 'pcg',\n",
              " 'meanings',\n",
              " 'mmol',\n",
              " 'gruppen',\n",
              " 'reserved',\n",
              " 'sopravvivenza',\n",
              " 'liberal',\n",
              " 'telemedicine',\n",
              " 'extremely',\n",
              " 'highlights',\n",
              " 'intramuscular',\n",
              " 'tag',\n",
              " 'insectivorous',\n",
              " 'blg',\n",
              " 'receiving',\n",
              " 'waveforms',\n",
              " 'prokaryotic',\n",
              " 'housed',\n",
              " 'albicans',\n",
              " 'reverse',\n",
              " 'cleave',\n",
              " 'nanotopography',\n",
              " 'novel',\n",
              " 'wenige',\n",
              " 'overdispersion',\n",
              " 'environment',\n",
              " 'nemo',\n",
              " 'dmvs',\n",
              " 'statement',\n",
              " 'aktuell',\n",
              " 'bei',\n",
              " 'pvr',\n",
              " 'informative',\n",
              " 'dixon',\n",
              " 'factors',\n",
              " 'effectively',\n",
              " 'purposes',\n",
              " 'scaled',\n",
              " 'repo',\n",
              " 'gie',\n",
              " 'eosinofila',\n",
              " 'conferencing',\n",
              " 'frijolito',\n",
              " 'nitroglycerin',\n",
              " 'expelled',\n",
              " 'presymptomatic',\n",
              " 'contrary',\n",
              " 'reference',\n",
              " 'vaccine',\n",
              " 'deutlich',\n",
              " 'coronaviral',\n",
              " 'flanked',\n",
              " 'jcv',\n",
              " 'gans',\n",
              " 'escalated',\n",
              " 'paraventricular',\n",
              " 'paving',\n",
              " 'analytes',\n",
              " 'yield',\n",
              " 'viscous',\n",
              " 'bones',\n",
              " 'führt',\n",
              " 'upregulation',\n",
              " 'computationally',\n",
              " 'druckentlastung',\n",
              " 'zie',\n",
              " 'electron',\n",
              " 'extended',\n",
              " 'capillaries',\n",
              " 'spillover',\n",
              " 'moi',\n",
              " 'rbps',\n",
              " 'behringwerke',\n",
              " 'mabs',\n",
              " 'ontology',\n",
              " 'already',\n",
              " 'ond',\n",
              " 'cytopathic',\n",
              " 'beachten',\n",
              " 'lübeck',\n",
              " 'predominance',\n",
              " 'respect',\n",
              " 'rely',\n",
              " 'interspersed',\n",
              " 'begleiten',\n",
              " 'rfp',\n",
              " 'levels',\n",
              " 'outbreak',\n",
              " 'investors',\n",
              " 'spätestens',\n",
              " 'mikrogefäßen',\n",
              " 'gig',\n",
              " 'inspections',\n",
              " 'erregerspektrum',\n",
              " 'possibly',\n",
              " 'copd',\n",
              " 'homogenates',\n",
              " 'isolated',\n",
              " 'kälbern',\n",
              " 'milano',\n",
              " 'breaks',\n",
              " 'conjugation',\n",
              " 'secrete',\n",
              " 'generation',\n",
              " 'rotational',\n",
              " 'reduces',\n",
              " 'decoction',\n",
              " 'iels',\n",
              " 'hpm',\n",
              " 'tobcns',\n",
              " 'matched',\n",
              " 'stereologisch',\n",
              " 'kindliche',\n",
              " 'hygiene',\n",
              " 'idle',\n",
              " 'yielding',\n",
              " 'satisfy',\n",
              " 'generators',\n",
              " 'jan',\n",
              " 'ideally',\n",
              " 'elbow',\n",
              " 'immunological',\n",
              " 'kobayashi',\n",
              " 'respiratoire',\n",
              " 'pps',\n",
              " 'chi',\n",
              " 'fluorescence',\n",
              " 'building',\n",
              " 'carriers',\n",
              " 'alpha',\n",
              " 'cylindrical',\n",
              " 'fcs',\n",
              " 'concentration',\n",
              " 'gruppe',\n",
              " 'audiologists',\n",
              " 'dringend',\n",
              " 'graue',\n",
              " 'aggregation',\n",
              " 'prostaglandin',\n",
              " 'oberflächenfärbung',\n",
              " 'advanced',\n",
              " 'extramedullary',\n",
              " 'vcds',\n",
              " 'tib',\n",
              " 'false',\n",
              " 'indotto',\n",
              " 'lithography',\n",
              " 'modelling',\n",
              " 'burkholderia',\n",
              " 'loss',\n",
              " 'ease',\n",
              " 'polymorphism',\n",
              " 'resonance',\n",
              " 'proportions',\n",
              " 'epileptogenic',\n",
              " 'arose',\n",
              " 'figured',\n",
              " 'neutrophileninfiltration',\n",
              " 'disinfect',\n",
              " 'furosemide',\n",
              " 'pyruvate',\n",
              " 'cald',\n",
              " 'hefti',\n",
              " 'supplementary',\n",
              " 'pdac',\n",
              " 'ebov',\n",
              " 'intracorneal',\n",
              " 'anhaltende',\n",
              " 'welfare',\n",
              " 'university',\n",
              " 'molybdate',\n",
              " 'antigenic',\n",
              " 'atom',\n",
              " 'neutrophil',\n",
              " 'genetics',\n",
              " 'reveals',\n",
              " 'untersuchten',\n",
              " 'nucleocapsids',\n",
              " 'trabeculectomy',\n",
              " 'besonders',\n",
              " 'ictv',\n",
              " 'synchronous',\n",
              " 'kinderheilkunde',\n",
              " 'circulating',\n",
              " 'endoribonuclease',\n",
              " 'dvt',\n",
              " 'mundial',\n",
              " 'hunching',\n",
              " 'carboxamide',\n",
              " 'agent',\n",
              " 'defenses',\n",
              " 'correspond',\n",
              " 'unified',\n",
              " 'southern',\n",
              " 'addressed',\n",
              " 'negative',\n",
              " 'antagonists',\n",
              " 'glioma',\n",
              " 'iκbα',\n",
              " 'notable',\n",
              " 'crazy',\n",
              " 'ouabain',\n",
              " 'kontrolle',\n",
              " 'dic',\n",
              " 'unconditional',\n",
              " 'nutrition',\n",
              " 'couple',\n",
              " 'glycosylation',\n",
              " 'cyclic',\n",
              " 'tur',\n",
              " 'anteil',\n",
              " 'door',\n",
              " 'puromycin',\n",
              " 'organspendern',\n",
              " 'circular',\n",
              " 'reveal',\n",
              " 'assemble',\n",
              " 'carbohydrate',\n",
              " 'association',\n",
              " 'gastrointestinal',\n",
              " 'arterielle',\n",
              " 'plastic',\n",
              " 'craters',\n",
              " 'comp',\n",
              " 'walsh',\n",
              " 'druggability',\n",
              " 'identification',\n",
              " 'hollow',\n",
              " 'asec',\n",
              " 'circuits',\n",
              " 'compressed',\n",
              " 'paramyxovirus',\n",
              " 'initiation',\n",
              " 'remove',\n",
              " 'mammals',\n",
              " 'complicated',\n",
              " 'columns',\n",
              " 'sdra',\n",
              " 'perceived',\n",
              " 'cnn',\n",
              " 'differenzierung',\n",
              " 'child',\n",
              " 'centres',\n",
              " 'penicillin',\n",
              " 'qin',\n",
              " 'microcirculation',\n",
              " 'automatic',\n",
              " 'hamburg',\n",
              " 'alters',\n",
              " 'confl',\n",
              " 'conversations',\n",
              " 'spiny',\n",
              " 'psyarxiv',\n",
              " 'testis',\n",
              " 'supporting',\n",
              " 'vwf',\n",
              " 'brilliant',\n",
              " 'substitutes',\n",
              " 'xenogeneic',\n",
              " 'constants',\n",
              " 'ein',\n",
              " 'tattoo',\n",
              " 'belief',\n",
              " 'pubmed',\n",
              " 'cholesterol',\n",
              " 'staining',\n",
              " 'plexus',\n",
              " 'uniform',\n",
              " 'fracture',\n",
              " 'communis',\n",
              " 'contribution',\n",
              " 'libraries',\n",
              " 'catalyse',\n",
              " 'homeostatic',\n",
              " 'spacers',\n",
              " 'condition',\n",
              " 'troke',\n",
              " 'rattus',\n",
              " 'enrichments',\n",
              " 'afectados',\n",
              " 'réduction',\n",
              " 'transl',\n",
              " 'rig',\n",
              " 'meditation',\n",
              " 'immunoaffinity',\n",
              " 'chips',\n",
              " 'flux',\n",
              " 'aiming',\n",
              " 'arrhythmia',\n",
              " 'phosphatase',\n",
              " 'melanoma',\n",
              " 'silico',\n",
              " 'hrp',\n",
              " 'limbal',\n",
              " 'heterotopic',\n",
              " 'cel',\n",
              " 'machining',\n",
              " 'mutate',\n",
              " 'absent',\n",
              " 'reconstruction',\n",
              " 'https',\n",
              " 'obstruktiven',\n",
              " 'masse',\n",
              " 'thermogravimetric',\n",
              " 'gly',\n",
              " 'foci',\n",
              " 'dsts',\n",
              " 'trimester',\n",
              " 'periods',\n",
              " 'regimen',\n",
              " 'aortic',\n",
              " 'eyestalk',\n",
              " 'moisturizer',\n",
              " 'apparatus',\n",
              " 'multibasic',\n",
              " 'athe',\n",
              " 'erde',\n",
              " 'ursache',\n",
              " 'hyperoxie',\n",
              " 'incorporates',\n",
              " 'surrounded',\n",
              " 'thema',\n",
              " 'kas',\n",
              " 'bisindigotin',\n",
              " 'plwh',\n",
              " 'parenchyma',\n",
              " 'exhaust',\n",
              " 'halo',\n",
              " 'faecium',\n",
              " 'stepover',\n",
              " 'cleaved',\n",
              " 'hypofolataemia',\n",
              " 'switch',\n",
              " 'appliances',\n",
              " 'phones',\n",
              " 'panel',\n",
              " 'exp',\n",
              " 'maximum',\n",
              " 'palosuo',\n",
              " 'zyste',\n",
              " 'scintigraphy',\n",
              " 'hdl',\n",
              " 'encounters',\n",
              " 'sampen',\n",
              " 'localities',\n",
              " 'koffein',\n",
              " 'tcr',\n",
              " 'einschließlich',\n",
              " 'unconfoundedness',\n",
              " 'deriving',\n",
              " ...)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Architecture**\n",
        "- **Input Layer:** One-hot encoded vector for target words of size 14171 (size ~ 10,000).\n",
        "- **Embedding Layer:** Dense layer that projects the input into a 300-dimensional space.\n",
        "- **Negative Sampling Layer:** Replaces naive softmax, selecting 5 negative samples.\n",
        "- **Output:** Context word prediction using the learned embeddings.\n"
      ],
      "metadata": {
        "id": "IhKy4aRZLApN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_skipgram_pairs(tokenized_corpus, window_size=2):\n",
        "    '''\n",
        "    Generate (target, context) pairs from a tokenized corpus for Skip-Gram model training.\n",
        "\n",
        "    Args:\n",
        "      tokenized_corpus: List of lists, where each list is a tokenized sentence/document.\n",
        "      window_size: The number of context words to consider on either side of the target word.\n",
        "\n",
        "    Returns:\n",
        "      List of tuples in the format (target, context).\n",
        "    '''\n",
        "    skipgram_pairs = []\n",
        "\n",
        "    for sentence in tokenized_corpus:\n",
        "        sentence_length = len(sentence)\n",
        "        for i, target_word in enumerate(sentence):\n",
        "            # Define the context window range\n",
        "            start = max(0, i - window_size)\n",
        "            end = min(sentence_length, i + window_size + 1)\n",
        "\n",
        "            # Collect context words for the target word\n",
        "            context_words = [sentence[j] for j in range(start, end) if j != i]\n",
        "\n",
        "            # Create (target, context) pairs\n",
        "            for context_word in context_words:\n",
        "                skipgram_pairs.append((target_word, context_word))\n",
        "\n",
        "    return skipgram_pairs"
      ],
      "metadata": {
        "id": "K1yM31-bKFXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate skipgram pairs\n",
        "skipgram_pairs = generate_skipgram_pairs(tokenized_corpus_filtered, window_size=2)\n",
        "print(f\"Generated {len(skipgram_pairs)} skipgram pairs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj-SuCfik4GB",
        "outputId": "b347eca8-ecc6-490e-f26e-4adfdc9367d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 1084536 skipgram pairs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(14)\n",
        "random.seed(14)\n",
        "np.random.seed(14)\n",
        "\n",
        "# this 'device' will be used for training our model\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "wFufFXGBCjwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a9e84b4-304f-4117-c202-24cdb1ec756a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGramNegSampling(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        '''\n",
        "        Initialize the Skip-Gram model with Negative Sampling.\n",
        "\n",
        "        Args:\n",
        "          vocab_size: The size of the vocabulary.\n",
        "          embedding_dim: The number of dimensions for the word embeddings.\n",
        "        '''\n",
        "        super(SkipGramNegSampling, self).__init__()\n",
        "        self.in_embeddings = nn.Embedding(vocab_size, embedding_dim)   # Input embedding matrix (W_in)\n",
        "        self.out_embeddings = nn.Embedding(vocab_size, embedding_dim)  # Output embedding matrix (W_out)\n",
        "\n",
        "    def forward(self, target_word, context_word, negative_words):\n",
        "        '''\n",
        "        Forward pass for Skip-Gram with Negative Sampling.\n",
        "\n",
        "        Args:\n",
        "          target_word: Tensor of target word indices.\n",
        "          context_word: Tensor of context word indices (positive samples).\n",
        "          negative_words: Tensor of negative word indices.\n",
        "\n",
        "        Returns:\n",
        "          Positive and negative logits for the loss computation.\n",
        "        '''\n",
        "        # Get embeddings for target, context (positive) and negative words\n",
        "        target_embedding = self.in_embeddings(target_word)  # Shape: (batch_size, embedding_dim)\n",
        "        context_embedding = self.out_embeddings(context_word)  # Shape: (batch_size, embedding_dim)\n",
        "        negative_embeddings = self.out_embeddings(negative_words)  # Shape: (batch_size, num_neg_samples, embedding_dim)\n",
        "\n",
        "        # Positive sample: dot product of target and context embeddings\n",
        "        pos_dot = torch.mul(target_embedding, context_embedding).sum(dim=1)  # Shape: (batch_size)\n",
        "\n",
        "        # Negative samples: dot product of target and negative word embeddings\n",
        "        neg_dot = torch.bmm(negative_embeddings, target_embedding.unsqueeze(2)).squeeze(2)  # Shape: (batch_size, num_neg_samples)\n",
        "\n",
        "        return pos_dot, neg_dot\n"
      ],
      "metadata": {
        "id": "p8PRRmZOKg9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def neg_sampling_loss(pos_dot, neg_dot):\n",
        "    '''\n",
        "    Computes the loss for negative sampling.\n",
        "\n",
        "    Args:\n",
        "      pos_dot: The dot product for positive samples (target, context).\n",
        "      neg_dot: The dot product for negative samples (target, negative words).\n",
        "\n",
        "    Returns:\n",
        "      The loss for the batch.\n",
        "    '''\n",
        "    # Positive samples loss: log(sigmoid(pos_dot))\n",
        "    pos_loss = F.logsigmoid(pos_dot).mean()\n",
        "\n",
        "    # Negative samples loss: log(sigmoid(-neg_dot))\n",
        "    neg_loss = F.logsigmoid(-neg_dot).mean()\n",
        "\n",
        "    # Total loss (negative sampling uses negative loss and positive loss together)\n",
        "    return -(pos_loss + neg_loss)\n"
      ],
      "metadata": {
        "id": "oOQcEbLhLads"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_negative_samples(vocab_size, batch_size, num_neg_samples):\n",
        "    '''\n",
        "    Generate negative samples for each target word in a batch.\n",
        "\n",
        "    Args:\n",
        "      vocab_size: The size of the vocabulary.\n",
        "      batch_size: The number of target words in the batch.\n",
        "      num_neg_samples: The number of negative samples per target word.\n",
        "\n",
        "    Returns:\n",
        "      Tensor of negative word indices (shape: batch_size x num_neg_samples).\n",
        "    '''\n",
        "    negative_words = np.random.choice(vocab_size, size=(batch_size, num_neg_samples), replace=True)\n",
        "    return torch.tensor(negative_words, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "nI94okl8_Rp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vocabulary and mapping of words to indices\n",
        "word_counts = Counter([word for sentence in tokenized_corpus_filtered for word in sentence])\n",
        "vocab = [word for word, count in word_counts.items()]\n",
        "vocab_size = len(vocab)\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
        "\n",
        "# Convert word pairs to indices\n",
        "def word_to_idx_pairs(skipgram_pairs):\n",
        "    return [(word_to_idx[target], word_to_idx[context]) for target, context in skipgram_pairs]\n",
        "\n",
        "# Skipgram pairs as indices\n",
        "skipgram_pairs_idx = word_to_idx_pairs(skipgram_pairs)"
      ],
      "metadata": {
        "id": "xAgiRRJfAe4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model with SGD"
      ],
      "metadata": {
        "id": "490jE4nh3ECI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "embedding_dim = 300  # Dimensionality of the word embeddings\n",
        "learning_rate = 1\n",
        "momentum = 0.9\n",
        "epochs = 20\n",
        "batch_size = 256\n",
        "num_neg_samples = 5  # Negative samples per positive pair"
      ],
      "metadata": {
        "id": "UxCzdM1LBWC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = SkipGramNegSampling(vocab_size, embedding_dim)\n",
        "model.to(device)\n",
        "\n",
        "# List to store the loss at each epoch\n",
        "training_errors = []\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)"
      ],
      "metadata": {
        "id": "AI8W1Mcu_wOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    random.shuffle(skipgram_pairs_idx)\n",
        "\n",
        "    for i in range(0, len(skipgram_pairs_idx), batch_size):\n",
        "        batch = skipgram_pairs_idx[i:i+batch_size]\n",
        "\n",
        "        # Move tensors to the device (GPU)\n",
        "        target_words = torch.tensor([pair[0] for pair in batch], dtype=torch.long, device=device)\n",
        "        context_words = torch.tensor([pair[1] for pair in batch], dtype=torch.long, device=device)\n",
        "        negative_words = generate_negative_samples(vocab_size, len(batch), num_neg_samples).to(device)\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        pos_dot, neg_dot = model(target_words, context_words, negative_words)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = neg_sampling_loss(pos_dot, neg_dot)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Average loss for the epoch\n",
        "    avg_loss = total_loss / len(skipgram_pairs_idx)\n",
        "    training_errors.append(avg_loss)  # Append the loss for this epoch\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}, Loss: {avg_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXUQyXLGGBm_",
        "outputId": "46dc9fa7-2209-4989-d960-9c6f26c644ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 0.028393103873003284\n",
            "Epoch: 2, Loss: 0.011318089698674495\n",
            "Epoch: 3, Loss: 0.0074379953411022265\n",
            "Epoch: 4, Loss: 0.0053514182455260935\n",
            "Epoch: 5, Loss: 0.004079532749904776\n",
            "Epoch: 6, Loss: 0.0032689572959027934\n",
            "Epoch: 7, Loss: 0.0027142819707315228\n",
            "Epoch: 8, Loss: 0.0023202036925532563\n",
            "Epoch: 9, Loss: 0.0020231895309343324\n",
            "Epoch: 10, Loss: 0.001804529079699014\n",
            "Epoch: 11, Loss: 0.0016255984251609815\n",
            "Epoch: 12, Loss: 0.0014854848756350237\n",
            "Epoch: 13, Loss: 0.0013620350228905826\n",
            "Epoch: 14, Loss: 0.0012721484414120498\n",
            "Epoch: 15, Loss: 0.0011951210398678433\n",
            "Epoch: 16, Loss: 0.0011226944400596738\n",
            "Epoch: 17, Loss: 0.001065949838911408\n",
            "Epoch: 18, Loss: 0.0010148558259315665\n",
            "Epoch: 19, Loss: 0.0009725454255907863\n",
            "Epoch: 20, Loss: 0.0009343335481041149\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Epochs vs Training Error (loss)"
      ],
      "metadata": {
        "id": "lFPt2i8M3Rzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the relationship between epochs and training error\n",
        "plt.plot(range(1, epochs+1), training_errors, marker='o', linestyle='-', color='b')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Training Error (Loss)')\n",
        "plt.title('Training Error vs. Number of Epochs')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a1Q4CJDKBuTV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "205be384-1320-4df4-ba3c-56045840bc50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model to a file\n",
        "with open('skipgram_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "print(\"Model saved as skipgram_model.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzWpCi2KYHjI",
        "outputId": "7342faeb-67b8-4b94-87d0-ff462ff84fea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as skipgram_model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing word analogies"
      ],
      "metadata": {
        "id": "uLBEfQ8D3eEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_embedding(word):\n",
        "  \"\"\"\n",
        "  Retrieves the word embedding for a given word.\n",
        "\n",
        "  Args:\n",
        "    word: The word for which to retrieve the embedding.\n",
        "\n",
        "  Returns:\n",
        "    The word embedding as a NumPy array, or None if the word is not found in the vocabulary.\n",
        "  \"\"\"\n",
        "  if word in word_to_idx:\n",
        "    word_idx = torch.tensor(word_to_idx[word], device=device)\n",
        "    word_embedding = model.in_embeddings(word_idx).detach().cpu()\n",
        "    return word_embedding\n",
        "  else:\n",
        "    return None"
      ],
      "metadata": {
        "id": "nN8wkN3cJ6LF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word analogy** : healthcare - prevention + phase = stage\n",
        "\n",
        "**Explanation**:\n",
        "\n",
        "- Removing prevention from healthcare shifts the focus away from preventive care.\n",
        "\n",
        "- Adding phase introduces the idea of a step in a process.\n",
        "- The result, stage, refers to a specific phase in a sequence, such as in treatment or disease progression.\n",
        "\n",
        "In short, it's about shifting from preventive healthcare to a phase-oriented view, resulting in a stage in the process.\n"
      ],
      "metadata": {
        "id": "4x0jTy8YyQ5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_1 = get_word_embedding('healthcare')\n",
        "embed_2 = get_word_embedding('prevention')\n",
        "embed_3 = get_word_embedding('phase')"
      ],
      "metadata": {
        "id": "M5FfEOjntb8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# healthcare - prevention + phase = stage\n",
        "a = embed_1 - embed_2 + embed_3"
      ],
      "metadata": {
        "id": "Jpl3Qa-dtpJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get word embedding and move it to CPU for NumPy operations\n",
        "# word_embedding = model.in_embeddings(a).detach().cpu()\n",
        "\n",
        "similarities = []\n",
        "for i in range(vocab_size):\n",
        "    # Make sure context index is a tensor on the correct device\n",
        "    context_idx = torch.tensor(i, device=device)\n",
        "\n",
        "    # Get context embedding and move it to CPU for NumPy operations\n",
        "    context_embedding = model.out_embeddings(context_idx).detach().cpu().numpy()\n",
        "\n",
        "    cosine_sim = np.dot(a, context_embedding) / (np.linalg.norm(a) * np.linalg.norm(context_embedding))\n",
        "    similarities.append((idx_to_word[i], cosine_sim))\n",
        "  # Sort by similarity\n",
        "similarities.sort(key=lambda x: x[1], reverse=True)"
      ],
      "metadata": {
        "id": "fFUPPZm5tvlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarities[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "DCaTLIIst6dx",
        "outputId": "17ced77e-aa7c-4dc5-f826-d707ea7aa266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'stage'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**word analogy** : coronavirus - covid + disease = infection\n",
        "\n",
        "**Explanation**:\n",
        "\n",
        "- coronavirus - covid: By subtracting covid (a specific disease caused by the coronavirus), we generalize the idea of coronavirus to any other diseases it may cause or represent, beyond just COVID-19\n",
        "\n",
        "- \\+ disease: Adding disease emphasizes the general concept of illness or health conditions caused by pathogens.\n",
        "\n",
        "- \\= infection: The result is infection, which refers to the process by which a disease spreads or develops in the body, applicable to many conditions, including but not limited to COVID-19.\n",
        "\n",
        "In summary, this analogy generalizes from covid (a specific disease) to infection, which is a broader concept in the context of diseases caused by pathogens."
      ],
      "metadata": {
        "id": "Fbq_Wb9EyhW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_1 = get_word_embedding('coronavirus')\n",
        "embed_2 = get_word_embedding('covid')\n",
        "embed_3 = get_word_embedding('disease')\n",
        "a = embed_1 - embed_2 + embed_3"
      ],
      "metadata": {
        "id": "vzUDfJb2yfAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get word embedding and move it to CPU for NumPy operations\n",
        "# word_embedding = model.in_embeddings(a).detach().cpu()\n",
        "\n",
        "similarities = []\n",
        "for i in range(vocab_size):\n",
        "    # Make sure context index is a tensor on the correct device\n",
        "    context_idx = torch.tensor(i, device=device)\n",
        "\n",
        "    # Get context embedding and move it to CPU for NumPy operations\n",
        "    context_embedding = model.out_embeddings(context_idx).detach().cpu().numpy()\n",
        "\n",
        "    cosine_sim = np.dot(a, context_embedding) / (np.linalg.norm(a) * np.linalg.norm(context_embedding))\n",
        "    similarities.append((idx_to_word[i], cosine_sim))\n",
        "  # Sort by similarity\n",
        "similarities.sort(key=lambda x: x[1], reverse=True)"
      ],
      "metadata": {
        "id": "yYYGvQtmzVuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarities[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "UcIJIjC5zcYA",
        "outputId": "7722e98a-9c85-4479-837b-c6637fa46453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'infection'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 5"
      ],
      "metadata": {
        "id": "dJ-0kc8Q2OcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chosen word : disease"
      ],
      "metadata": {
        "id": "_qQWs6ad6gkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"disease\"\n",
        "embedding = get_word_embedding(word)\n",
        "if embedding is not None:\n",
        "  print(f\"Word embedding for '{word}':\\n{embedding}\")\n",
        "else:\n",
        "  print(f\"Word '{word}' not found in the vocabulary.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHhYfAO6Qu9k",
        "outputId": "4bdf2cf3-00a0-4abc-9c36-4c5025540135"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word embedding for 'disease':\n",
            "tensor([ 6.5200e-01, -1.9716e-01, -1.2590e-01, -4.0400e-01,  1.8954e-01,\n",
            "         6.9817e-02,  7.5008e-02, -4.9190e-01,  4.0385e-02, -1.5249e-01,\n",
            "         6.3250e-02,  9.7713e-02,  4.6347e-01, -2.3879e-01, -2.5807e-01,\n",
            "         3.0021e-01, -3.7631e-01,  3.1456e-02, -1.2739e-02,  2.5579e-01,\n",
            "         2.1644e-02,  4.9756e-01,  1.4811e-01,  1.7095e-02, -3.8120e-02,\n",
            "        -7.4219e-01, -7.8383e-01,  8.5902e-01, -2.7667e-01,  1.6380e-01,\n",
            "         4.4468e-01, -9.3250e-02,  3.0034e-01, -5.1525e-01,  3.8722e-01,\n",
            "        -2.0226e-01,  5.0168e-04,  3.1773e-01, -7.0817e-02,  1.2967e-02,\n",
            "        -5.0259e-01,  1.4460e-01,  2.0826e-02,  4.0038e-01, -3.2045e-02,\n",
            "        -2.0684e-01,  3.0113e-01,  4.5830e-01,  1.8988e-01,  3.7187e-02,\n",
            "         2.4395e-01,  1.1505e-01, -1.6506e-01, -5.9458e-01, -1.2655e-01,\n",
            "        -2.9628e-01,  2.0720e-02,  3.3842e-01,  1.4236e-01,  2.8563e-01,\n",
            "        -1.3446e-01, -4.5762e-01, -4.3459e-01,  6.4435e-01, -7.5540e-01,\n",
            "        -3.9554e-01, -7.7868e-01,  1.8257e-01, -1.8597e-01, -3.8475e-01,\n",
            "        -5.5310e-01, -2.3707e-02,  3.1660e-01,  5.8117e-01,  3.0620e-01,\n",
            "         4.2573e-01, -2.4758e-01, -3.5359e-01, -2.3329e-01,  5.3495e-01,\n",
            "         3.5243e-02,  1.1468e-01,  4.4650e-01, -3.9046e-01,  1.6076e-01,\n",
            "        -5.3056e-02, -1.3018e-01,  3.1684e-01, -4.0846e-01, -3.2564e-02,\n",
            "        -3.9729e-01,  8.4562e-01,  5.3939e-01,  8.8433e-01,  7.3192e-02,\n",
            "         2.3580e-01, -3.2832e-01,  4.8927e-01,  3.0543e-01,  4.8903e-01,\n",
            "         1.4374e-02,  4.5016e-01,  5.7294e-01,  1.5091e-02,  8.3753e-03,\n",
            "         1.4023e-01,  1.0030e-01,  5.3536e-01,  1.1860e-01, -1.2720e-01,\n",
            "        -5.5289e-02,  3.2396e-01,  1.9016e-01, -2.9354e-01, -4.8861e-01,\n",
            "         2.0569e-01, -5.9435e-01, -6.2213e-01, -6.5750e-01, -8.6898e-01,\n",
            "        -3.9327e-02,  7.9894e-01,  1.1032e-01, -3.0318e-01,  5.0779e-01,\n",
            "         5.9518e-01, -6.7438e-02, -4.5522e-01, -4.0691e-01, -1.8868e-01,\n",
            "         2.0372e-03,  3.9129e-01, -5.0378e-01, -1.5625e-01, -8.6639e-02,\n",
            "         4.3986e-01,  4.1239e-03,  9.7617e-02,  4.6182e-01, -4.6623e-01,\n",
            "        -1.3138e-01,  6.9266e-01,  4.1152e-01,  3.2825e-01, -4.0499e-01,\n",
            "         6.0619e-01, -9.8386e-02,  6.4454e-02, -1.8632e-01, -7.6022e-02,\n",
            "        -2.2083e-01, -1.4198e-01, -6.1139e-02,  2.6621e-01,  6.2553e-01,\n",
            "        -1.6111e-01,  6.0933e-01, -5.9598e-01, -5.0742e-01, -2.9310e-01,\n",
            "        -8.5900e-01, -6.7496e-01, -1.3965e-02, -4.0998e-01,  2.6263e-02,\n",
            "         6.9329e-02, -2.7744e-02,  1.4301e-01, -1.5932e-01,  4.7222e-01,\n",
            "        -9.7480e-02, -8.9213e-01,  8.5667e-01,  4.7938e-01, -4.7161e-01,\n",
            "        -2.1681e-01,  4.1057e-01, -3.4444e-01, -4.3263e-01, -8.9569e-02,\n",
            "        -3.3285e-01, -5.6671e-01, -1.5321e-01,  2.3381e-01, -1.1861e-01,\n",
            "         2.1425e-01,  6.0238e-01,  2.6662e-01,  2.3932e-01, -3.5237e-02,\n",
            "        -5.7628e-02,  7.5502e-01, -4.5760e-01,  8.7798e-01,  5.6554e-01,\n",
            "        -9.1110e-01, -5.4088e-02,  2.1519e-01, -3.0539e-01, -5.9514e-01,\n",
            "         9.9218e-02,  4.1674e-01,  7.2083e-01, -1.8532e-01, -4.0605e-02,\n",
            "         3.6086e-02,  7.0530e-01, -5.7636e-01, -8.6908e-01, -4.3810e-01,\n",
            "         8.2265e-01,  1.9367e-01,  3.5782e-01,  3.4652e-01,  3.4318e-02,\n",
            "        -2.4231e-01,  5.6142e-01, -1.1489e-01, -2.9905e-01, -2.2345e-01,\n",
            "        -6.2490e-02,  1.3115e+00, -2.4319e-01, -3.0320e-02,  3.9530e-01,\n",
            "        -3.0978e-01,  4.7359e-01,  7.0234e-01,  2.2888e-01, -1.6482e-01,\n",
            "        -2.8765e-01, -2.2867e-01,  4.2870e-02, -1.8317e-01, -2.6567e-01,\n",
            "         3.4078e-01,  2.0213e-01, -2.1824e-02,  3.0522e-01, -4.1252e-01,\n",
            "         7.5079e-01, -3.1141e-01, -5.9241e-03, -4.1713e-01, -2.5240e-01,\n",
            "        -5.9852e-01, -2.9577e-01,  1.0367e-01, -6.0578e-02, -3.8238e-02,\n",
            "        -1.4969e-01,  6.3163e-02,  4.4995e-02, -5.8055e-01,  1.2780e-01,\n",
            "        -2.3509e-02, -1.5764e-01, -8.8722e-01,  6.7697e-02,  9.7237e-01,\n",
            "         1.2175e-01, -5.2179e-01,  2.8750e-01,  6.1395e-01,  4.5069e-01,\n",
            "        -4.6107e-01,  3.5661e-01,  4.5095e-01, -5.6437e-02,  3.0629e-01,\n",
            "        -2.0669e-01,  3.2029e-01,  2.0868e-01,  2.7015e-01, -6.5879e-01,\n",
            "         3.6693e-01,  5.3576e-02, -5.3331e-01,  1.0521e-01, -3.3900e-01,\n",
            "         1.1472e-01,  4.0053e-01, -2.0511e-01,  1.1720e-01,  4.0836e-01,\n",
            "         3.0072e-02,  7.3457e-02,  1.7032e-01, -7.3946e-02, -2.8256e-01,\n",
            "        -1.9352e-01, -2.4723e-02,  1.8243e-01, -2.2485e-01,  1.9917e-01,\n",
            "        -3.6281e-01, -9.5632e-03, -4.5658e-01, -4.4090e-01,  3.4138e-02])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding similar words to the word 'disease' using $W_{in}$"
      ],
      "metadata": {
        "id": "sYS8eWzhZbS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similar_words_using_W_in(word, word_to_index, index_to_word, W_in, top_n=5):\n",
        "    '''\n",
        "    Finds the top-N similar words for a given word using the W_in matrix.\n",
        "\n",
        "    Args:\n",
        "      word: The word for which we want to find similar words.\n",
        "      word_to_index: A dictionary mapping words to their indices.\n",
        "      index_to_word: A dictionary mapping indices to their corresponding words.\n",
        "      W_in: The input matrix (word embedding matrix).\n",
        "      top_n: The number of similar words to return (default is 5).\n",
        "\n",
        "    Returns:\n",
        "      A list of tuples containing the similar words and their cosine similarity scores.\n",
        "    '''\n",
        "    word_idx = word_to_idx[word]\n",
        "    word_embedding = W_in[word_idx].reshape(1, -1)  # Get the embedding for the given word\n",
        "\n",
        "    # Calculate cosine similarity between the word and all other words\n",
        "    similarities = cosine_similarity(word_embedding, W_in)[0]\n",
        "\n",
        "    # Get the top-N most similar words\n",
        "    similar_indices = similarities.argsort()[-top_n-1:-1][::-1]  # Sort and get top-n indices\n",
        "    similar_words = [(idx_to_word[idx], similarities[idx]) for idx in similar_indices]\n",
        "\n",
        "    return similar_words\n"
      ],
      "metadata": {
        "id": "OaTVNeH5Sr04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'disease'\n",
        "similar_words_W_in = get_similar_words_using_W_in(word, word_to_idx[word], idx_to_word[word_to_idx[word]], W_in = model.in_embeddings.weight.data.cpu(), top_n=5)\n",
        "print(f\"Top similar words for '{word}' using W_in: {similar_words_W_in}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjIyWKAamvwL",
        "outputId": "ccaa8355-6c98-4bb5-ba7f-2582d2e76650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top similar words for 'disease' using W_in: [('diseases', 0.4130399), ('inflammatory', 0.34420758), ('infections', 0.34399664), ('risk', 0.32892263), ('acute', 0.3206243)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding similar words to the word 'disease' using $W_{out}$"
      ],
      "metadata": {
        "id": "rWS7754kZt2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similar_words_using_W_out(word, word_to_index, index_to_word, W_out, top_n=5):\n",
        "    '''\n",
        "    Finds the top-N similar words for a given word using the W_out matrix.\n",
        "\n",
        "    Args:\n",
        "      word: The word for which we want to find similar words.\n",
        "      word_to_index: A dictionary mapping words to their indices.\n",
        "      index_to_word: A dictionary mapping indices to their corresponding words.\n",
        "      W_out: The output matrix.\n",
        "      top_n: The number of similar words to return (default is 5).\n",
        "\n",
        "    Returns:\n",
        "      A list of tuples containing the similar words and their cosine similarity scores.\n",
        "    '''\n",
        "    word_idx = word_to_idx[word]\n",
        "    word_embedding = W_out[word_idx].reshape(1, -1)  # Get the embedding for the given word\n",
        "\n",
        "    # Calculate cosine similarity between the word and all other words\n",
        "    similarities = cosine_similarity(word_embedding, W_out)[0]\n",
        "\n",
        "    # Get the top-N most similar words\n",
        "    similar_indices = similarities.argsort()[-top_n-1:-1][::-1]  # Sort and get top-n indices\n",
        "    similar_words = [(idx_to_word[idx], similarities[idx]) for idx in similar_indices]\n",
        "\n",
        "    return similar_words\n"
      ],
      "metadata": {
        "id": "BJmbsOMkYccQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "similar_words_W_out = get_similar_words_using_W_out(word, word_to_idx, idx_to_word, W_out = model.out_embeddings.weight.data.cpu(), top_n=5)\n",
        "print(f\"Top similar words for '{word}' using W_out: {similar_words_W_out}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV08uC8JZxlJ",
        "outputId": "0b881e63-583f-4cb5-93a0-2e9dd5c68336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top similar words for 'disease' using W_out: [('patients', 0.36471528), ('infection', 0.3211676), ('viral', 0.3168927), ('virus', 0.29496276), ('used', 0.28112215)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find similar words to the word 'disease' using $W_{avg} = \\frac{W_{in} + W_{out}}{2}$"
      ],
      "metadata": {
        "id": "-jry4tGoZyvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similar_words_avg(word, word_to_index, index_to_word, W_in, W_out, top_n=5):\n",
        "    '''\n",
        "    Finds the top-N similar words for a given word using the average of W_in and W_out matrices.\n",
        "\n",
        "    Args:\n",
        "      word: The word for which we want to find similar words.\n",
        "      word_to_index: A dictionary mapping words to their indices.\n",
        "      index_to_word: A dictionary mapping indices to their corresponding words.\n",
        "      W_in: The input matrix.\n",
        "      W_out: The output matrix.\n",
        "      top_n: The number of similar words to return (default is 5).\n",
        "\n",
        "    Returns:\n",
        "      A list of tuples containing the similar words and their cosine similarity scores.\n",
        "    '''\n",
        "    word_idx = word_to_idx[word]\n",
        "\n",
        "    # Average the W_in and W_out embeddings\n",
        "    combined_embedding = ((W_in[word_idx] + W_out[word_idx]) / 2).reshape(1, -1)\n",
        "\n",
        "    # Calculate cosine similarity between the word and all other words\n",
        "    avg_matrix = (W_in + W_out) / 2\n",
        "    similarities = cosine_similarity(combined_embedding, avg_matrix)[0]\n",
        "\n",
        "    # Get the top-N most similar words\n",
        "    similar_indices = similarities.argsort()[-top_n-1:-1][::-1]  # Sort and get top-n indices\n",
        "    similar_words = [(idx_to_word[idx], similarities[idx]) for idx in similar_indices]\n",
        "\n",
        "    return similar_words"
      ],
      "metadata": {
        "id": "J60bsuFsaPuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similar_words_avg = get_similar_words_avg(word, word_to_idx, idx_to_word, W_in = model.in_embeddings.weight.data.cpu(), W_out = model.out_embeddings.weight.data.cpu(), top_n=5)\n",
        "print(f\"Top similar words for '{word}' using averaged W_in and W_out: {similar_words_avg}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEgt-zT3aIrt",
        "outputId": "0bad385d-d19b-46c5-8df0-2dd90c2f00ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top similar words for 'disease' using averaged W_in and W_out: [('diseases', 0.4694792), ('patients', 0.40887433), ('severe', 0.40584725), ('infection', 0.38363713), ('lung', 0.3803318)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing the results for the word 'disease'"
      ],
      "metadata": {
        "id": "IMLHuME9aKtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Results using W_in: {similar_words_W_in}\")\n",
        "print(f\"Results using W_out: {similar_words_W_out}\")\n",
        "print(f\"Results using combined W_in and W_out: {similar_words_avg}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIZcUqVDaySr",
        "outputId": "a4994df9-2335-4f67-b2e7-a0bb0209e127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results using W_in: [('diseases', 0.4130399), ('inflammatory', 0.34420758), ('infections', 0.34399664), ('risk', 0.32892263), ('acute', 0.3206243)]\n",
            "Results using W_out: [('patients', 0.36471528), ('infection', 0.3211676), ('viral', 0.3168927), ('virus', 0.29496276), ('used', 0.28112215)]\n",
            "Results using combined W_in and W_out: [('diseases', 0.4694792), ('patients', 0.40887433), ('severe', 0.40584725), ('infection', 0.38363713), ('lung', 0.3803318)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Description of the outcome\n",
        "\n",
        "The results by $W_{in}$, $W_{out}$ and $W_{avg}$ were all different for the word 'disease'. Though all had words that were somewhat related to the context of disease.\n",
        "\n",
        "The results using each matrix differ because the word embeddings in $W_{in}$, $W_{out}$, and $W_{avg}$ capture different aspects of the relationships between words. For example:\n",
        "\n",
        "- $W_{in}$ gives words related to \"disease\" such as \"diseases\" and \"infections\".\n",
        "\n",
        "- $W_{out}$ returns less intuitively connected words like \"used\" and \"viral\", along with more connected words like \"patients\" and \"infection\", possibly because it captures how \"disease\" is used as context rather than the target.\n",
        "\n",
        "- $W_{avg}$ balances both input and output embeddings, giving words like \"severe\" and \"lung\", which might reflect a broader relationship to \"disease.\"\n",
        "\n",
        "---\n",
        "\n",
        "### Explanation of Results\n",
        "- $W_{in}$ captures words that appear as targets of the word \"disease.\" Words like \"diseases\" and \"infections\" are logically related since they appear in similar contexts in medical documents.\n",
        "- $W_{out}$ returns words like \"patients\" and \"infection\". This might reflect how \"disease\" is often used in proximity to these words in the corpus, even if they are less directly related.\n",
        "- $W_{avg}$ balances both input and output embeddings, hence providing words that are somewhat related to both target and context words, such as \"dieases\", \"patients\" and \"severe\".\n"
      ],
      "metadata": {
        "id": "iHwjj8Hpr3xi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Complexities of skipgram and CBOW model\n",
        "As per the slide related to the complexities of skipgram and CBOW model shown in the class:\n",
        "\n",
        "Complexity of skipgram with negative sampling: $O(T \\cdot N \\cdot C \\cdot k)$\n",
        "\n",
        "Complexity of CBOW with negative sampling: $O(T \\cdot N \\cdot C \\cdot k)$\n",
        "\n",
        "where\n",
        "\n",
        "$T$ is total number of words in the corpus\n",
        "\n",
        "$V$ is vocabulary size\n",
        "\n",
        "$N$ is embedding size\n",
        "\n",
        "$C$ is context window size\n",
        "\n",
        "$k$ is number of negative samples\n",
        "\n",
        "---\n",
        "\n",
        "### Complexity Breakdown\n",
        "The complexity $O(T \\cdot N \\cdot C \\cdot k)$ arises from the following:\n",
        "- \\(T\\): The number of words in the corpus, since the model must iterate through every word in training.\n",
        "- \\(N\\): The embedding size (dimensionality of the word vectors). For each word, we need to compute \\(N\\)-dimensional vector operations.\n",
        "- \\(C\\): The context window size, since we must predict for each word within the window.\n",
        "- \\(k\\): The number of negative samples, since negative sampling approximates the softmax by updating only \\(k\\) negative word vectors for each training example."
      ],
      "metadata": {
        "id": "Qwyjq-49p5Nk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Justifying the Computational Complexity $O(T \\cdot N \\cdot C \\cdot k)$ for Skipgram and CBOW Models with Negative Sampling\n",
        "\n",
        "The complexity of the **Skipgram** and **Continuous Bag of Words (CBOW)** models with **negative sampling** can be broken down by analyzing each of the components involved in the training process. Let's dive into the roles of $T$, $N$, $C$, and $k$ to understand why the overall complexity is $O(T \\cdot N \\cdot C \\cdot k)$.\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Total Number of Words in the Corpus (\\(T\\))**\n",
        "- **Reasoning**: The model processes the entire corpus, iterating through every word during training.\n",
        "- Both **Skipgram** and **CBOW** models are trained by iterating through each word in the training corpus.\n",
        "- For each word, we either predict its surrounding context words (Skipgram) or use the surrounding context to predict the word itself (CBOW).\n",
        "  \n",
        "Thus, the number of words in the corpus directly affects the number of training steps, which is proportional to \\(T\\).\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Embedding Size (\\(N\\))**\n",
        "- **Reasoning**: The dimensionality of word vectors (or embeddings) impacts the cost of each forward and backward pass.\n",
        "  \n",
        "In both models, each word is represented as a dense vector of size \\(N\\). For each prediction:\n",
        "- **Skipgram**: The input word's embedding is used to predict the context words.\n",
        "- **CBOW**: The context words' embeddings are used to predict the target word.\n",
        "\n",
        "For each prediction, vector operations like dot products, gradient computations, and updates are performed on \\(N\\)-dimensional vectors. Hence, the number of computations per training step scales with the embedding size \\(N\\), as we need to update and compute gradients for embeddings in a space of dimensionality \\(N\\).\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Context Window Size (\\(C\\))**\n",
        "- **Reasoning**: The model's complexity grows linearly with the number of context words considered.\n",
        "\n",
        "In both models, the **context window size (\\(C\\))** defines how many words are used as context for each target word:\n",
        "- **Skipgram**: For each target word, the model predicts each of its \\(C\\) surrounding words.\n",
        "  - For example, if the window size is 5, the model will try to predict the 5 context words around the target word. Thus, the larger the context window size, the more predictions need to be made for each word, increasing computational cost.\n",
        "- **CBOW**: For each group of \\(C\\) context words, the model predicts the central word. The larger the context window, the more embeddings are summed (or averaged) to form the input for prediction.\n",
        "  \n",
        "Hence, the cost scales linearly with \\(C\\) because each word (Skipgram) or target word prediction (CBOW) is associated with \\(C\\) surrounding words.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Number of Negative Samples (\\(k\\))**\n",
        "- **Reasoning**: Negative sampling reduces the computational cost of softmax, but still requires \\(k\\) negative samples per prediction.\n",
        "\n",
        "In **negative sampling**, instead of computing the full softmax over the entire vocabulary (which would involve updating embeddings for every word in the vocabulary), the model only updates embeddings for:\n",
        "- The true positive context word(s).\n",
        "- \\(k\\) randomly chosen negative samples.\n",
        "\n",
        "Thus, instead of a full softmax over \\(V\\) words (where \\(V\\) is the vocabulary size), the model computes the dot product and updates the parameters for only \\(k+1\\) words (1 positive word + \\(k\\) negative samples).\n",
        "\n",
        "The number of negative samples, \\(k\\), dictates how many random non-context words are chosen and updated during each training step. Since each of these samples involves vector operations (dot product, gradients, etc.), the computational cost is proportional to \\(k\\).\n",
        "\n",
        "---\n",
        "\n",
        "### Putting It All Together: $O(T \\cdot N \\cdot C \\cdot k)$\n",
        "\n",
        "1. **\\(T\\)**: The model iterates through every word in the corpus (number of words = \\(T\\)).\n",
        "2. **\\(N\\)**: Each word is represented as a dense vector of size \\(N\\), and the operations (dot products, updates) are performed on these vectors.\n",
        "3. **\\(C\\)**: For each word, the model predicts \\(C\\) context words (Skipgram) or uses \\(C\\) context words to predict the target word (CBOW).\n",
        "4. **\\(k\\)**: For each target-context word pair, the model uses negative sampling to approximate the softmax, updating parameters for \\(k+1\\) words (1 positive + \\(k\\) negatives).\n",
        "\n",
        "Thus, the overall complexity per epoch is:\n",
        "$\n",
        "O(T \\cdot N \\cdot C \\cdot k)\n",
        "$\n",
        "where:\n",
        "- **\\(T\\)** determines how many training steps we perform.\n",
        "- **\\(N\\)** determines the cost of each vector operation (embedding lookup, dot product, gradient update).\n",
        "- **\\(C\\)** determines how many context words we consider for each target word (in Skipgram) or how many words we sum for predicting the target word (in CBOW).\n",
        "- **\\(k\\)** determines the number of negative samples for each positive prediction, directly affecting the number of operations to update the embeddings.\n",
        "\n",
        "<!-- --- -->\n",
        "\n",
        "<!-- ### Why is Negative Sampling Efficient?\n",
        "\n",
        "Without negative sampling, the complexity would be proportional to \\(V\\), where \\(V\\) is the vocabulary size (since we would need to compute and normalize probabilities for every word in the vocabulary). This would lead to a complexity of $O(T \\cdot N \\cdot C \\cdot V)$, which is computationally infeasible for large vocabularies.\n",
        "\n",
        "By introducing negative sampling, the model only needs to update a small subset of the vocabulary during each training step (1 positive word and \\(k\\) negative words), reducing the complexity to $O(T \\cdot N \\cdot C \\cdot k)$, where $k$ is much smaller than $V$.\n",
        "\n",
        "Negative sampling provides a trade-off between approximation accuracy and efficiency, allowing large models to be trained on vast corpora with reasonable computational resources. -->\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "The complexity $O(T \\cdot N \\cdot C \\cdot k)$ reflects how the Skipgram and CBOW models scale with:\n",
        "- The size of the corpus (T),\n",
        "- The dimensionality of the word embeddings (N),\n",
        "- The number of context words used for each target word (C),\n",
        "- The number of negative samples used during training (k).\n",
        "\n",
        "This balance allows word embedding models to be trained efficiently on large datasets without computing a full softmax over a large vocabulary, thanks to negative sampling.\n"
      ],
      "metadata": {
        "id": "R99KYJBW_GVy"
      }
    }
  ]
}
